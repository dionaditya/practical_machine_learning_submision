---
title: "Machine learning course project"
output:
  html_document: default
  pdf_document: default
---

# Project introduction
### Background
##### Using devices such as Jawbone Up, Nike FuelBand, and Fitbit It has become relatively inexpensive to collect large amounts of data about an individual's activities. These types of devices are part of a quantified self-exercise??? To improve your health or find patterns in your behavior or because you are a technician, measure yourself regularly group of enthusiasts doing One thing that people do regularly is quantify how well they are doing a particular activity, but rarely how well they are doing it. The goal of this project is to use data from the belt, forearm, arm, and dumbbell accelerometers of six participants. They were asked to correctly perform five different barbell lifts. For more information, please visit his website: http://groupware.les.inf.puc-rio.br/har (see weightlifting exercise dataset section).

### Data
##### The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
##### The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
##### The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

### Assignment
##### The goal of your project is to predict the manner in which you exercised This is the 'classe' variable in the training set. You can predict using other variables. You should produce a report that explains how you built your model, how you used cross-validation, what your expected out-of-sample error was, and why you made your decisions. We also use the predictive model to predict 20 different test cases.

# Getting and Cleaning Data
### Load library
```{r, message=FALSE, warning=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)
```
### Getting Data
```{r echo=TRUE}
training_data <- read.csv("pml-training.csv")
testing_data <- read.csv("pml-testing.csv")
inTrain <- createDataPartition(training_data$classe, p=0.6, list=FALSE)
myTraining <- training_data[inTrain, ]
myTesting <- training_data[-inTrain, ]
```
### Cleaning Data
```{r echo=TRUE}
# remove variables with nearly zero variance
nzv <- nearZeroVar(myTraining)
myTraining <- myTraining[, -nzv]
myTesting <- myTesting[, -nzv]

# remove variables that are almostly NA
mostlyNA <- sapply(myTraining, function(x) mean(is.na(x))) > 0.95
myTrainig <- myTraining[, mostlyNA==F]
myTesting <- myTesting[, mostlyNA==F]

# remove identification only variables (columns 1 to 5)
myTraining <- myTrainig[, -(1:5)]
myTesting  <- myTesting[, -(1:5)]
```
# Predict Data by various models
### 1. Random forest
```{r echo=TRUE}
modFit <- randomForest(classe ~ ., data=myTraining)
modFit

# Prediction using Random forest
predict <- predict(modFit, myTesting, type="class")
confusionMatrix(myTesting$classe, predict)
```
### 2. Decision tree
```{r echo=TRUE}
modFit_T <- rpart(classe~., myTraining)

# Prediction using Decision tree
predict_T <- predict(modFit_T, myTesting, type="class")
confusionMatrix(myTesting$classe, predict_T)
```
### 3. Generalized Boosted Model (GBM)
```{r, message=FALSE, warning=FALSE}
control_GBM <- trainControl(method = "repeatedcv", number=5, repeats=1)
modFit_GBM <- train(classe~., myTraining, method="gbm", trControl=control_GBM, verbose=FALSE)
```
```{r echo=TRUE}
# Prediction using GBM
predict_GBM <- predict(modFit_GBM, myTesting)
confusionMatrix(predict_GBM, myTesting$classe)
```

# Error and Cross validation
#### Random forest, Dicision tree, and GBM models give us 99.6 %, 75.4 %, and 98.8 % as accuracy, respectively.
#### The expected sample errors for Random forest, Dicision tree, and GBM are 0.4 %, 24.6 %, and 1.2 %, respectively.

# Final test
#### Run the algorithm to the 20 test cases in the test data using most accurate model Random forest.
```{r echo=TRUE}
predict_test <- predict(modFit, testing_data, type = "class")
predict_test
```